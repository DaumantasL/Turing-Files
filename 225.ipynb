{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "225.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pYG9nEfG3vn"
      },
      "source": [
        "# Module 2: Data Engineering\n",
        "## Sprint 2: SQL and Data Scraping\n",
        "## Storing Scraped Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rWvI0yxG3vq"
      },
      "source": [
        "## Background \n",
        "You did an outstanding job this sprint. Now you know SQL databases, how to make basic data operations. You are familiar with the pros and cons of SQL and should know when to choose a NoSQL database. Data scraping is a technique that you can use to create your own dataset. You are also familiar with the basic concepts of Spark. You should be proud of yourself! For the last lesson of this week, you should put all your learnings into one place to collect, process, and store data. Combining these two might require a bit of planning at first but this is what you will need to do now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J1zIb8uG3vr"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl4Kve-mG3vr"
      },
      "source": [
        "## Creating the PostgreSQL database\n",
        "First, you will need to create a database that you will need to use to store the data you will collect. Follow the steps provided in the second lesson of this sprint. Do not forget to remove all secrets and passwords when committing code to the repository. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILFvK-5bG3vs"
      },
      "source": [
        "## Scraping the data\n",
        "This lesson, you will need to scrape and store data collected from an online store. You can choose any website that you like (Vinted, Amazon, eBay, etc.). You will need to select three keywords (for example `dress`, `bike`, `bracelet`) and scrape listings from the selected website. You will need to collect at least 3000 samples for each category and store this information of the listing: `category`, `title`, `price`, `url to item`, `url of image`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q2a3nLCG3vs"
      },
      "source": [
        "## Structure of the database\n",
        "As you will be storing data into the relational database, you will need to create tables. There should be two tables with many to one type of relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5B8vv7jG3vt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4h5mHZG3vt"
      },
      "source": [
        "## Concepts to explore\n",
        "* Creating relational type database\n",
        "* Creating PostgreSQL database in Heroku\n",
        "* Writing SQL queries\n",
        "* Scraping webpages using Beautiful Soup\n",
        "* Storing and acquiring data using SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzUs7lT7G3vu"
      },
      "source": [
        "## Requirements\n",
        "You should create two files: one that contains functions related to data scraping and another that is created to enable communication to PostgreSQL database created with Heroku. The actual requirements are these:\n",
        "- Database should be created using Heroku.\n",
        "- Required tables should be created. Python code should be provided inside `.py` file.\n",
        "- Scraping functions should be created. The main scraping function should take two arguments: `number of examples to scrape` and `keyword to search`. The main scraping function should return a Pandas `DataFrame` with the records.\n",
        "- Scrape the website. Get minimum 3000 samples of each category (keyword)\n",
        "- Data should be inserted into tables of database hosted by Heroku. Provide screenshots proofing that data sits inside the database.\n",
        "- Join two tables into one using SQL query and export it to `csv` file. Provide a function that makes this action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhZ7yC4sThSl",
        "outputId": "0b72c5fa-3596-4794-cc83-d4962e1ca272"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rNQiC89RCp0",
        "outputId": "e98f7320-7432-4f6d-d6d7-7d5aa6b8eb87"
      },
      "source": [
        "pip install fake-useragent"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13503 sha256=45db01d312053363540a3fcbc6d2dd4bcc6fb1548d0d1b0ddfd77a56ae1e4712\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILlyxERPUFO"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHaPMAtNvAbd"
      },
      "source": [
        "def reed_scraper(job_name: str, examples_total: int) -> pd.DataFrame:\n",
        "  '''Scrapes unique reed job listings matching keyword.'''\n",
        "  results = set()\n",
        "  ua = UserAgent()\n",
        "  parsed_job_name = job_name.strip().replace(\" \", \"-\")\n",
        "  dfcolumn_names = [\"position\", \"pay\", \"employer\", \"location\", \"county\", \"desc_short\", \"jobtype\", \"remote\", \"joburl\", \"jobrefID\"]\n",
        "  data_remaining = True\n",
        "  page = 0\n",
        "\n",
        "\n",
        "  def newpage_setup() -> BeautifulSoup:\n",
        "    '''Soup object for new page'''\n",
        "    header = {'User-Agent':str(ua.random)}\n",
        "    url = f\"https://www.reed.co.uk/jobs/{parsed_job_name}-jobs?pageno={page}&sortby=DisplayDate\"\n",
        "    request = requests.get(url, headers=header)\n",
        "    chicken_stock = BeautifulSoup(request.content, \"html.parser\")\n",
        "    return chicken_stock\n",
        "\n",
        "\n",
        "  def rip_page() -> [set, bool]:\n",
        "    '''Extract data from all unique job listings on page.'''\n",
        "    page_results = set()\n",
        "    soup = newpage_setup()\n",
        "    joblistings = soup.find_all('article', class_=\"job-result\")\n",
        "    last_page = len(joblistings) < 25\n",
        "    job_index = 0\n",
        "\n",
        "    while ( ( (len(results) + len(page_results)) < examples_total ) & (job_index < len(joblistings)) ):\n",
        "      page_results.add(rip_line (joblistings[job_index]) )\n",
        "      job_index += 1\n",
        "      \n",
        "    time.sleep(1.5)\n",
        "    return [page_results, last_page]\n",
        "\n",
        "    \n",
        "  def rip_line(job: 'bs4.element.ResultSet') -> tuple:\n",
        "    '''Extract data from a single job listing.'''\n",
        "\n",
        "    position = job.find('a', {\"data-gtm\": \"job_click\"}).text\n",
        "\n",
        "    pay = job.find('li', class_=\"salary\").text\n",
        "\n",
        "    employer = job.find('a', class_=\"gtmJobListingPostedBy\").text\n",
        "\n",
        "    location_string_generator = job.find('li', class_=\"location\").stripped_strings\n",
        "    location = next(location_string_generator)\n",
        "    county = next(location_string_generator, location)\n",
        "\n",
        "    desc_short = job.find('div', class_=\"description\").find('p').text\n",
        "\n",
        "    jobtype = job.find('li', class_=\"time\").text\n",
        "\n",
        "    remote = not (job.find('li', class_=\"remote\") is None)\n",
        "\n",
        "    joburl = job.find('a', {\"data-gtm\": \"job_click\"}).get('href')\n",
        "    joburl = \"https://www.reed.co.uk\" + joburl.split(\"?\", 1)[0]\n",
        "    jobrefID = int (joburl.split('/')[-1])\n",
        "\n",
        " #  spoon_request = requests.get(joburl, headers=header)\n",
        " #  soup_spoon = BeautifulSoup(spoon_request.content, \"html.parser\")\n",
        " #  full_desc = soup_spoon.find('span', {\"itemprop\": \"description\"}).get_text(separator=\"|\")\n",
        " #  employer_img = soup_spoon.find('a', {\"data-qa\": \"recruiterLogoLnk\"}).find('img').attrs['data-src']\n",
        "\n",
        "    rippedline = [\n",
        "                 position,\n",
        "                 pay,\n",
        "                 employer,\n",
        "                 location,\n",
        "                 county,\n",
        "                 desc_short,\n",
        "                 jobtype,\n",
        "                 remote,\n",
        "                 joburl,\n",
        "                 jobrefID\n",
        "                 ]\n",
        "    \n",
        "    return tuple(rippedline)\n",
        "  \n",
        "\n",
        "  while ( (len(results) < examples_total) & (data_remaining) ):\n",
        "    page_state = rip_page()\n",
        "    results.update(page_state[0])\n",
        "    data_remaining = not page_state[1]\n",
        "    page += 1\n",
        "\n",
        "  if (not data_remaining) & (len(results) < examples_total): print(\"No more unique job listings remaining to scrape.\")\n",
        "  \n",
        "  return pd.DataFrame(results, columns=dfcolumn_names, copy=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "ZdsVyuD7rbai",
        "outputId": "44f1cff3-2c1e-431a-b916-a6eb9e855460"
      },
      "source": [
        "data_scientist_df = reed_scraper(\"data scientist\", 3000)\n",
        "data_scientist_df.tail()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No more unique job listings remaining to scrape.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>position</th>\n",
              "      <th>pay</th>\n",
              "      <th>employer</th>\n",
              "      <th>location</th>\n",
              "      <th>county</th>\n",
              "      <th>desc_short</th>\n",
              "      <th>jobtype</th>\n",
              "      <th>remote</th>\n",
              "      <th>joburl</th>\n",
              "      <th>jobrefID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>Early Detection Research Programme Manager</td>\n",
              "      <td>Competitive salary</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>North East England</td>\n",
              "      <td>North East England</td>\n",
              "      <td>Early Detection Research Programme ManagerThis...</td>\n",
              "      <td>Permanent, full-time</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.reed.co.uk/jobs/early-detection-re...</td>\n",
              "      <td>43460301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>Formulation Scientist</td>\n",
              "      <td>Salary negotiable</td>\n",
              "      <td>PE Global</td>\n",
              "      <td>Nottingham</td>\n",
              "      <td>Nottinghamshire</td>\n",
              "      <td>PE Global is currently recruiting for a Formul...</td>\n",
              "      <td>Contract, full-time</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.reed.co.uk/jobs/formulation-scient...</td>\n",
              "      <td>43203982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>Finance Analyst Apprentice</td>\n",
              "      <td>£20,000 per annum, inc benefits</td>\n",
              "      <td>Multiverse</td>\n",
              "      <td>London</td>\n",
              "      <td>London</td>\n",
              "      <td>A new opportunity has arisen in Charles Taylor...</td>\n",
              "      <td>Permanent, full-time</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.reed.co.uk/jobs/finance-analyst-ap...</td>\n",
              "      <td>43139610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>Analytics Manager Data Science</td>\n",
              "      <td>£50,000 - £60,000 per annum</td>\n",
              "      <td>Cactus Search</td>\n",
              "      <td>The Home</td>\n",
              "      <td>Shropshire</td>\n",
              "      <td>Our Client is looking to source an experienced...</td>\n",
              "      <td>Permanent, full-time</td>\n",
              "      <td>False</td>\n",
              "      <td>https://www.reed.co.uk/jobs/analytics-manager-...</td>\n",
              "      <td>43270273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>Business Analyst - Digital Health Start-Up</td>\n",
              "      <td>Salary negotiable</td>\n",
              "      <td>Zest Business Group</td>\n",
              "      <td>London</td>\n",
              "      <td>London</td>\n",
              "      <td>Zest Scientific is partnered with one of the U...</td>\n",
              "      <td>Contract, full-time</td>\n",
              "      <td>True</td>\n",
              "      <td>https://www.reed.co.uk/jobs/business-analyst-d...</td>\n",
              "      <td>43256002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       position  ...  jobrefID\n",
              "727  Early Detection Research Programme Manager  ...  43460301\n",
              "728                       Formulation Scientist  ...  43203982\n",
              "729                  Finance Analyst Apprentice  ...  43139610\n",
              "730              Analytics Manager Data Science  ...  43270273\n",
              "731  Business Analyst - Digital Health Start-Up  ...  43256002\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwmIq_VgXF41"
      },
      "source": [
        "data_scientist_df.to_csv(\"data scientist\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9dmEh-G3vu"
      },
      "source": [
        "## Evaluation criteria\n",
        "1. The requirements are met (database and tables in it are created, data is scraped and stored)\n",
        "2. Data scraping functions are written.\n",
        "3. Code meets expected standards (type hints, PEP8 standards)\n",
        "4. Documentation is provided (comments are written where needed, README.md file is created)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8fY7s57G3vv"
      },
      "source": [
        "\n",
        "## Sample correction questions\n",
        "\n",
        "During a correction, you may get asked questions that test your understanding of covered topics.\n",
        "\n",
        "- Why do we store information inside external databases? Tell two advantages and disadvantages when comparing local and external storage.\n",
        "- What is the difference between SQL and NoSQL databases? Tell two examples: one where you would want to use PostgreSQL database and another where you would choose Cassandra.\n",
        "- How to properly set up a data scraping strategy? What are the key steps that you must make to successfully create a dataset using web scraping technique?\n",
        "- Why do we need Spark? Tell about one use case where Spark would improve scalability and speed of queries when comparing to traditional relational type database's engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDrTHljhG3vv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}